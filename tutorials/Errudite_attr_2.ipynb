{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f13445f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def import_sys():\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "import_sys()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9984c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from overrides import overrides\n",
    "from typing import List\n",
    "from errudite.io import DatasetReader\n",
    "from errudite.utils import normalize_file_path, accuracy_score\n",
    "from errudite.targets.instance import Instance\n",
    "from errudite.targets.target import Target\n",
    "from errudite.targets.label import Label, PredefinedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b2b4a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register STE as DatasetReader: Overwritting name already in use for STEReader.\n"
     ]
    }
   ],
   "source": [
    "@DatasetReader.register(\"STE\")\n",
    "class STEReader(DatasetReader):\n",
    "    def __init__(self, cache_folder_path: str=None) -> None:\n",
    "        super().__init__(cache_folder_path)\n",
    "        # overwrite the primary evaluation method and metric name\n",
    "        Label.set_task_evaluator(accuracy_score, 'accuracy')\n",
    "        \n",
    "    @overrides\n",
    "    def _read(self, file_path: str, lazy: bool, sample_size: int) -> List[Instance]:\n",
    "        \"\"\"\n",
    "        Returns a list containing all the instances in the specified dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            The path of the input data file.\n",
    "        lazy : bool\n",
    "            If ``lazy==True``, only run the tokenization, does not compute the linguistic\n",
    "            features like POS, NER. By default False\n",
    "        sample_size : int\n",
    "            If sample size is set, only load this many of instances, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Instance]\n",
    "            The instance list.\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        \n",
    "        # Read data from the CSV file\n",
    "        df = pd.read_csv(normalize_file_path(file_path))\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            instance = self._text_to_instance(f'q:{idx}', row)\n",
    "            if instance is not None:\n",
    "                instances.append(instance)\n",
    "            if sample_size and idx >= sample_size:\n",
    "                break\n",
    "\n",
    "        return instances\n",
    "    \n",
    "    @overrides\n",
    "    def _text_to_instance(self, id: str, row) -> Instance:\n",
    "        # The function that transfers raw text to instance.\n",
    "        essay = Target(qid=id, text=row['Essay'], vid=0, metas={'type': 'essays'})        # label\n",
    "        groundtruth_PE = PredefinedLabel(\n",
    "            model='groundtruth', \n",
    "            qid=id,\n",
    "            text=row['PE_actual'], \n",
    "            vid=0, \n",
    "        )\n",
    "        groundtruth_KE = PredefinedLabel(\n",
    "            model='groundtruth', \n",
    "            qid=id,\n",
    "            text=row['KE_actual'], \n",
    "            vid=0, \n",
    "        )\n",
    "        groundtruth_LCE = PredefinedLabel(\n",
    "            model='groundtruth', \n",
    "            qid=id,\n",
    "            text=row['LCE_actual'], \n",
    "            vid=0, \n",
    "        )\n",
    "        prediction_PE = PredefinedLabel(\n",
    "            model='prediction', \n",
    "            qid=id,\n",
    "            text=row['PE_predicted'], \n",
    "            vid=0, \n",
    "        )\n",
    "        prediction_KE = PredefinedLabel(\n",
    "            model='prediction', \n",
    "            qid=id,\n",
    "            text=row['KE_predicted'], \n",
    "            vid=0, \n",
    "        )\n",
    "        prediction_LCE = PredefinedLabel(\n",
    "            model='prediction', \n",
    "            qid=id,\n",
    "            text=row['LCE_predicted'], \n",
    "            vid=0, \n",
    "        )\n",
    "        return self.create_instance(row['Essay'], \n",
    "            essay=essay, \n",
    "            groundtruth_PE=groundtruth_PE,\n",
    "            groundtruth_KE=groundtruth_KE,\n",
    "            groundtruth_LCE=groundtruth_LCE,\n",
    "            prediction_PE=prediction_PE,\n",
    "            prediction_KE=prediction_KE,\n",
    "            prediction_LCE=prediction_LCE\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "922ee2ad-01c3-46b8-938d-bfeed0bcff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# import torch\n",
    "# from typing import List, Dict\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from errudite.predictors.predictor import Predictor\n",
    "# from errudite.targets.label import Label, PredefinedLabel\n",
    "\n",
    "# class Predictor_FLAN_T5(Predictor):\n",
    "#     def __init__(self, name: str, \n",
    "#     model_path: str=None,\n",
    "#     model_online_path: str=None,\n",
    "#     description: str='',\n",
    "#     model_type: str=None) -> None:\n",
    "#         model = None\n",
    "#         Predictor.__init__(self, name, description, model, ['accuracy', 'accuracy_PE_Acceptable', 'accuracy_PE_Unacceptable'\n",
    "#                                                             ,'accuracy_KE_Acceptable', 'accuracy_KE_Unacceptable',\n",
    "#                                                            'accuracy_LCE_Acceptable', 'accuracy_LCE_Unacceptable'])\n",
    "        \n",
    "#     def predict_essay(self, essay : str) -> Dict[str,str]:\n",
    "#         # Load data into pandas DataFrame\n",
    "#         data_df = pd.read_excel(\"StudentEssays.xlsx\")\n",
    "\n",
    "#         # Initialize T5 tokenizer and model\n",
    "#         tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "#         model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "#         # Move the model to the CUDA device if available\n",
    "#         if torch.cuda.is_available():\n",
    "#             model.to(\"cuda\")\n",
    "\n",
    "#         # Define a list of concepts to predict\n",
    "#         concepts_to_predict = [\"potential energy\", \"kinetic energy\", \"Law of Conservation of Energy\"]\n",
    "\n",
    "#         # Define possible outcome labels\n",
    "#         outcome_labels = [\"Acceptable\", \"Unacceptable\"]\n",
    "\n",
    "#         # Create a list to store predictions as dictionaries\n",
    "#         predictions_list = []\n",
    "\n",
    "#         text = essay  # Assuming the text content is in column 'Essay'\n",
    "\n",
    "#         # Initialize predictions dictionary for this row\n",
    "#         predictions = {}\n",
    "\n",
    "#         # Iterate through each concept to predict\n",
    "#         for concept in concepts_to_predict:\n",
    "#             # Define a template for classification\n",
    "#             template = f\"According to the following essay, is the student's definition of {concept} Acceptable, Unacceptable, Insufficient, or Not Found? Only use one of these labels for outputs\\n{text}\"\n",
    "#             # Prepare the input by replacing placeholders\n",
    "#             formatted_input = template\n",
    "#             # Tokenize and classify the text\n",
    "#             input_ids = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#             outputs = model.generate(input_ids, max_length=128)\n",
    "#             decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Remove special tokens\n",
    "\n",
    "#             # Store the prediction in the dictionary\n",
    "#             predictions[concept] = next((label for label in outcome_labels if label.lower() in decoded_output.lower()), \"Unknown\")\n",
    "\n",
    "#             if predictions[concept] == \"Unknown\":\n",
    "#               print(len(decoded_output))\n",
    "#               with open('output.txt', 'w') as f:\n",
    "#                 f.write(decoded_output)\n",
    "\n",
    "#         return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e52fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.utils.file_utils:Errudite cache folder selected: ./ste_caches\n"
     ]
    }
   ],
   "source": [
    "from errudite.io import DatasetReader\n",
    "\n",
    "cache_folder_path = \"./ste_caches\"\n",
    "reader = DatasetReader.by_name(\"STE\")(cache_folder_path=cache_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc8db182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.io.dataset_reader:Reading instances from lines in file at: 5fcvResults.csv\n",
      "INFO:__main__:Reading instances from lines in file at: 5fcvResults.csv\n",
      "76it [00:02, 28.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the raw data!\n",
    "instances = reader.read(\n",
    "    # The path of the input data file. We are using the first 100 rows from the SNLI dev set.\n",
    "    file_path='5fcvResults.csv', \n",
    "    # If sample size is set, only load this many of instances, by default None.\n",
    "    sample_size=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa56a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from errudite.predictors.predictor import Predictor\n",
    "from errudite.targets.label import Label, PredefinedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7dd5e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register STE_FLAN_T5 as Predictor: Overwritting name already in use for PredictorSTE.\n"
     ]
    }
   ],
   "source": [
    "@Predictor.register(\"STE_FLAN_T5\")\n",
    "class PredictorSTE(Predictor_FLAN_T5):\n",
    "    def __init__(self, name: str, \n",
    "        model_path: str=None,\n",
    "        model_online_path: str=None,\n",
    "        description: str='') -> None:\n",
    "        Predictor_FLAN_T5.__init__(self, name, model_path, model_online_path, description)\n",
    "        import sys\n",
    "        sys.path.append('..')\n",
    "        from errudite.utils.evaluator import accuracy_score\n",
    "        # Second, from the metrics above, pick one that's primary, and it will be used \n",
    "        # to compute `is_incorrect()` in any label target object: primary metric < 1.\n",
    "        Label.set_task_evaluator(\n",
    "            # the evaluation function that accepts pred and groundtruths, \n",
    "            # and return a dict of metrics: { metric_name: metric_score }. \n",
    "            # This is saved as Label.task_evaluation_func.\n",
    "            task_evaluation_func=accuracy_score, \n",
    "            # The primary task metric name, ideally a key of task_evaluation_func â€˜s return.\n",
    "            task_primary_metric='accuracy')\n",
    "\n",
    "    # the raw prediction function, returning the output of the model in a json format.\n",
    "    def predict(self, essay: str) -> Dict[str, str]:\n",
    "        predicted = self.predict_essay(essay)\n",
    "        return predicted\n",
    "\n",
    "    @classmethod\n",
    "    # the class method that takes `Target` inputs, and output a `Label` object.\n",
    "    def model_predict(cls, \n",
    "        predictor: Predictor, \n",
    "        essay: Target, \n",
    "        groundtruth_PE: Label, groundtruth_KE: Label, groundtruth_LCE: Label) -> 'Label':\n",
    "        answer = None\n",
    "        if not predictor:\n",
    "            return answer\n",
    "        predicted = predictor.predict(essay.get_text())\n",
    "        if not predicted:\n",
    "            return None\n",
    "        answer_PE = PredefinedLabel(\n",
    "            model=predictor.name, \n",
    "            qid=essay.qid,\n",
    "            text=predicted['potential energy'], \n",
    "            vid=max([essay.vid]))\n",
    "        answer_KE = PredefinedLabel(\n",
    "            model=predictor.name, \n",
    "            qid=essay.qid,\n",
    "            text=predicted['kinetic energy'], \n",
    "            vid=max([essay.vid]))\n",
    "        answer_LCE = PredefinedLabel(\n",
    "            model=predictor.name, \n",
    "            qid=essay.qid,\n",
    "            text=predicted['Law of Conservation of Energy'], \n",
    "            vid=max([essay.vid]))\n",
    "        answer_PE.compute_perform(groundtruths=groundtruth_PE)\n",
    "        answer_KE.compute_perform(groundtruths=groundtruth_KE)\n",
    "        answer_LCE.compute_perform(groundtruths=groundtruth_LCE)\n",
    "        return answer_PE, answer_KE, answer_LCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4688a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.predictors import Predictor\n",
    "model_path = \"FLAN_T5_Essay\"\n",
    "predictor = Predictor.by_name(\"STE_FLAN_T5\")(\n",
    "    name ='FLAN_T5', \n",
    "    description='Prediction created by FLAN_T5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "623786d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import Union, List\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "def import_sys():\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "import_sys()\n",
    "from errudite.utils.helpers import convert_doc\n",
    "from errudite.utils.check import DSLValueError\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "215e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm \n",
    "# logger.info(\"Running predictions....\") \n",
    "# for instance in tqdm(instances):\n",
    "#     prediction_PE, prediction_KE, prediction_LCE = Predictor.by_name(\"STE_FLAN_T5\").model_predict(\n",
    "#         predictor, \n",
    "#         essay = instance.essay,\n",
    "#         groundtruth_PE = instance.groundtruth_PE, groundtruth_KE = instance.groundtruth_KE, groundtruth_LCE = instance.groundtruth_LCE)\n",
    "#     # set the prediction\n",
    "#     instance.set_entries(prediction_PE = prediction_PE, prediction_KE = prediction_KE, prediction_LCE = prediction_LCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e07bd2-fec7-42fb-ba9a-6eac87da25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(group):\n",
    "\n",
    "    cu0_count=0\n",
    "    cu5_count=0\n",
    "    len_group=0\n",
    "    for key in group.get_instances():\n",
    "        len_group+=1\n",
    "        instance =Instance.get(key)\n",
    "        if(int(instance.get_entry(\"groundtruth_cu0\").label)== int(instance.get_entry(\"predict_cu0\").label)):\n",
    "            cu0_count+=1 \n",
    "        if(int(instance.get_entry(\"groundtruth_cu5\").label)== int(instance.get_entry(\"predict_cu5\").label)):\n",
    "            cu5_count+=1 \n",
    "    cu0_acc=cu0_count/len_group\n",
    "    cu5_acc=cu5_count/len_group\n",
    "    print(f'cu0 accuracy: {cu0_acc}\\ncu5_accuracy: {cu5_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9bc04530-9447-4c34-8421-0e7f334cbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(group) -> None:\n",
    "        \"\"\"Save the performance of the predictor.\n",
    "        It iterates through metric names in ``self.perform_metrics``, and average the \n",
    "        corresponding metrics in ``instance.prediction.perform``. It saves the results\n",
    "        in ``self.perform``.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        instances : List[Instance]\n",
    "            The list of instances, with predictions from this model already saved as\n",
    "            part of its entries.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            The result is saved in ``self.perform``.\n",
    "        \"\"\"\n",
    "        pred_PE_accept_num=0\n",
    "        pred_PE_unaccept_num=0\n",
    "        pred_KE_accept_num=0\n",
    "        pred_KE_unaccept_num=0\n",
    "        pred_LCE_accept_num=0\n",
    "        pred_LCE_unaccept_num=0\n",
    "        ground_PE_accept_num=0\n",
    "        ground_PE_unaccept_num=0\n",
    "        ground_KE_accept_num=0\n",
    "        ground_KE_unaccept_num=0\n",
    "        ground_LCE_accept_num=0\n",
    "        ground_LCE_unaccept_num=0\n",
    "        for key in group.get_instances():\n",
    "\n",
    "            #len_group+=1\n",
    "            instance =Instance.get(key)\n",
    "            \n",
    "            #PE\n",
    "            #unacceptable\n",
    "            if(instance.get_entry('prediction_PE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                pred_PE_unaccept_num+=1\n",
    "                if(instance.get_entry('groundtruth_PE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                    ground_PE_unaccept_num+=1\n",
    "            #acceptable\n",
    "            if(instance.get_entry('prediction_PE').label == \"acceptable\"):\n",
    "                pred_PE_accept_num+=1\n",
    "                if(instance.get_entry('groundtruth_PE').label == \"acceptable\"):\n",
    "                    ground_PE_accept_num+=1\n",
    "\n",
    "\n",
    "            #KE\n",
    "            #unacceptable\n",
    "            if(instance.get_entry('prediction_KE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                pred_KE_unaccept_num+=1\n",
    "                if(instance.get_entry('groundtruth_KE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                    ground_KE_unaccept_num+=1\n",
    "            #acceptable\n",
    "            if(instance.get_entry('prediction_KE').label == \"acceptable\"):\n",
    "                pred_KE_accept_num+=1\n",
    "                if(instance.get_entry('groundtruth_KE').label == \"acceptable\"):\n",
    "                    ground_KE_accept_num+=1\n",
    "\n",
    "            #LCE\n",
    "            #unacceptable\n",
    "            if(instance.get_entry('prediction_LCE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                pred_LCE_unaccept_num+=1\n",
    "                if(instance.get_entry('groundtruth_LCE').label in [\"insufficient\",\"not found\",\"unacceptable\"]):\n",
    "                    ground_LCE_unaccept_num+=1\n",
    "            #acceptable\n",
    "            if(instance.get_entry('prediction_LCE').label == \"acceptable\"):\n",
    "                pred_LCE_accept_num+=1\n",
    "                if(instance.get_entry('groundtruth_LCE').label == \"acceptable\"):\n",
    "                    ground_LCE_accept_num+=1\n",
    "\n",
    "        if pred_PE_unaccept_num==0:\n",
    "            PE_unaccept_acc=\"None\"\n",
    "        else: \n",
    "            PE_unaccept_acc=ground_PE_unaccept_num/pred_PE_unaccept_num\n",
    "\n",
    "        if pred_PE_accept_num==0:\n",
    "            PE_accept_acc=\"None\"\n",
    "        else:\n",
    "            PE_accept_acc=ground_PE_accept_num/pred_PE_accept_num\n",
    "        print(f'PE_acceptable accuracy: {PE_accept_acc} PE_unacceptable accuracy: {PE_unaccept_acc}')\n",
    "\n",
    "\n",
    "        if pred_KE_unaccept_num==0:\n",
    "            KE_unaccept_acc=\"None\"\n",
    "        else: \n",
    "            KE_unaccept_acc=ground_KE_unaccept_num/pred_KE_unaccept_num\n",
    "\n",
    "        if pred_KE_accept_num==0:\n",
    "            KE_accept_acc=\"None\"\n",
    "        else:\n",
    "            KE_accept_acc=ground_KE_accept_num/pred_KE_accept_num\n",
    "\n",
    "        print(f'KE_acceptable accuracy: {KE_accept_acc} KE_unacceptable accuracy: {KE_unaccept_acc}')\n",
    "\n",
    "        if pred_LCE_unaccept_num==0:\n",
    "            LCE_unaccept_acc=\"None\"\n",
    "        else: \n",
    "            LCE_unaccept_acc=ground_LCE_unaccept_num/pred_LCE_unaccept_num\n",
    "\n",
    "        if pred_LCE_accept_num==0:\n",
    "            LCE_accept_acc=\"None\"\n",
    "        else:\n",
    "            LCE_accept_acc=ground_LCE_accept_num/pred_LCE_accept_num\n",
    "        print(f'LCE_acceptable accuracy: {LCE_accept_acc} LCE_unacceptable accuracy: {LCE_unaccept_acc}')\n",
    "        \n",
    "        print(pred_PE_accept_num)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a35deb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(instances[2].get_entry('essay'), \"\\n\")\n",
    "# instances[2].show_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "70ded76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Build the instance store hash\n",
    "from errudite.targets.instance import Instance\n",
    "instance_hash, instance_hash_rewritten, qid_hash = Instance.build_instance_hashes(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b641f149",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-b731398479d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"predictor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"perform\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/errudite/errudite/predictors/predictor.py\u001b[0m in \u001b[0;36mevaluate_performance\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mcount_correct_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                         \u001b[0mpe_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_PE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                         \u001b[0mke_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_KE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0mlce_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_LCE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "predictor.evaluate_performance(instances)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06ad7",
   "metadata": {},
   "source": [
    "## Group - Length of the Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3cb631da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[FuncOp(length):[ArgOp:essay]+[], 10.0]\n",
      "WARNING:errudite.utils.store:Storing length in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: length\n"
     ]
    }
   ],
   "source": [
    "from errudite.builts import Group\n",
    "from errudite.builts import Attribute\n",
    "group_length_10 = Group.create(\n",
    "    # The name of the attribute\n",
    "    name=\"length\",\n",
    "    # the description of the attribute\n",
    "    description=\"length greater than 10\",\n",
    "    # All the previously created attributes and groups \n",
    "    # can be used and queried, as long as we serve the \n",
    "    # stored attributes and groups as part of the inputs.\n",
    "    cmd=\"length(essay) > 10\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d36a4dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counts': {'correct': 0, 'incorrect': 76},\n",
       " 'stats': {'coverage': 1.0,\n",
       "  'error_coverage': 1.0,\n",
       "  'local_error_rate': 1.0,\n",
       "  'global_error_rate': 1.0}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some general stats on groups\n",
    "Group.eval_stats(\n",
    "    filtered_instances=group_length_10.get_instances(),\n",
    "    # this will automatically call the default model we got\n",
    "    model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5a9d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[FuncOp(length):[ArgOp:essay]+[], 200.0]\n",
      "WARNING:errudite.utils.store:Storing length in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: length\n"
     ]
    }
   ],
   "source": [
    "from errudite.builts import Group\n",
    "from errudite.builts import Attribute\n",
    "group_length_200 = Group.create(\n",
    "    # The name of the attribute\n",
    "    name=\"length\",\n",
    "    # the description of the attribute\n",
    "    description=\"length greater than 200\",\n",
    "    # All the previously created attributes and groups \n",
    "    # can be used and queried, as long as we serve the \n",
    "    # stored attributes and groups as part of the inputs.\n",
    "    cmd=\"length(essay) > 200\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cf7cf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v3+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v3.2.1.json",
       "config": {
        "mark": {
         "tooltip": null
        },
        "view": {
         "height": 300,
         "width": 400
        }
       },
       "data": {
        "name": "data-16dc8f24917c1cb442088268770311db"
       },
       "datasets": {
        "data-16dc8f24917c1cb442088268770311db": [
         {
          "correctness": "correct",
          "count": 0,
          "model": "FLAN_T5"
         },
         {
          "correctness": "incorrect",
          "count": 33,
          "model": "FLAN_T5"
         }
        ]
       },
       "encoding": {
        "color": {
         "field": "correctness",
         "scale": {
          "domain": [
           "correct",
           "incorrect"
          ]
         },
         "type": "nominal"
        },
        "tooltip": [
         {
          "field": "model",
          "type": "nominal"
         },
         {
          "field": "count",
          "type": "quantitative"
         },
         {
          "field": "correctness",
          "type": "nominal"
         }
        ],
        "x": {
         "field": "count",
         "stack": "zero",
         "type": "quantitative"
        },
        "y": {
         "field": "model",
         "type": "nominal"
        }
       },
       "mark": "bar",
       "width": 100
      },
      "image/png": "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",
      "text/plain": [
       "<VegaLite 3 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_length_200.visualize_models(models=[\"FLAN_T5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93beb608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counts': {'correct': 0, 'incorrect': 33},\n",
       " 'stats': {'coverage': 0.4342105263157895,\n",
       "  'error_coverage': 0.4342105263157895,\n",
       "  'local_error_rate': 1.0,\n",
       "  'global_error_rate': 0.4342105263157895}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some general stats on groups\n",
    "Group.eval_stats(\n",
    "    filtered_instances=group_length_200.get_instances(),\n",
    "    # this will automatically call the default model we got\n",
    "    model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe8089",
   "metadata": {},
   "source": [
    "## Group - Quantitative Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "010498f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register contains_quantity as PrimFunc: Overwritting name already in use for contains_quantity.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def contains_quantity(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the presence of quantity entities in the essay.\n",
    "    quantity entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "    #print(docs)\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "    # print(entities)\n",
    "    # print(docs)\n",
    "    contains='QUANTITY' in entities\n",
    "\n",
    "    # Check if 'bottom' or 'top' is present in the extracted entity types\n",
    "    #print(contains)\n",
    "    return contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2582db11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_quantity):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing quantity_entities in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: quantity_entities\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):quantity_entities, True]\n",
      "WARNING:errudite.utils.store:Storing quantity in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: quantity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](quantity): \n",
       "\tCMD\t: attr:quantity_entities == TRUE\n",
       "\tCOUNT\t: 49"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"quantity_entities\",\n",
    "    description=\"Presence of quantity entities in the essay\",\n",
    "    cmd=\"contains_quantity(essay)\"\n",
    ")\n",
    "\n",
    "# Create a group that checks for the presence of location entities\n",
    "quantity_group = Group.create(\n",
    "    name=\"quantity\",\n",
    "    description=\"quantity entity detected\",\n",
    "    cmd=\"attr:quantity_entities == TRUE\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "quantity_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ca66672e-740b-4f48-a4ff-740a1a89f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE_acceptable accuracy: None PE_unacceptable accuracy: 0.8775510204081632\n",
      "KE_acceptable accuracy: 0.0 KE_unacceptable accuracy: 0.875\n",
      "LCE_acceptable accuracy: 0.7692307692307693 LCE_unacceptable accuracy: 0.9722222222222222\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(quantity_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9cd7b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-523f41dc5f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount_group_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_group_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"predictor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"perform\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/errudite/errudite/predictors/predictor.py\u001b[0m in \u001b[0;36mevaluate_performance\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mcount_correct_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                         \u001b[0mpe_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_PE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                         \u001b[0mke_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_KE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0mlce_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction_LCE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "\n",
    "for key in quantity_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "239a802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_quantity):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing quantity_entities in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: quantity_entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Lower Quartile : 0.0\n",
      "median : 2.0\n",
      "Upper Quartile : 4.0\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_quantity(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the number of quantity entities in the essay.\n",
    "    quantity entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "    #print(docs)\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "    count = entities.count('QUANTITY')\n",
    "\n",
    "    return count\n",
    "\n",
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"quantity_entities\",\n",
    "    description=\"number of quantity entities in the essay\",\n",
    "    cmd=\"num_quantity(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a26c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](<=):[[BuildBlockOp](attr):quantity_entities, 0.0]\n",
      "INFO:errudite.builts.group:Created group: num_quantity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](num_quantity): \n",
       "\tCMD\t: attr:quantity_entities <= 0 \n",
       "\tCOUNT\t: 27"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a group that checks for the presence of location entities\n",
    "num_quantity_group = Group.create(\n",
    "    name=\"num_quantity\",\n",
    "    description=\"quantity entity detected\",\n",
    "    cmd=\"attr:quantity_entities <= 0 \",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "num_quantity_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adbfa777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.1111111111111111, 'accuracy_PE_Acceptable': 'None', 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.88, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.5909090909090909, 'accuracy_KE_NotFound': 0.0, 'accuracy_LCE_Acceptable': 0.35714285714285715, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.4}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "\n",
    "for key in num_quantity_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6053252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def contains_ordinal(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the presence of ordinal entities in the essay.\n",
    "    ordinal entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "    # print(entities)\n",
    "    # print(docs)\n",
    "    contains='ORDINAL' in entities\n",
    "    # Check if 'bottom' or 'top' is present in the extracted entity types\n",
    "    #print(contains)\n",
    "    return contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d87c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_ordinal):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: ordinal_entities\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):ordinal_entities, True]\n",
      "INFO:errudite.builts.group:Created group: ordinal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](quantity): \n",
       "\tCMD\t: attr:quantity_entities == TRUE\n",
       "\tCOUNT\t: 49"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"ordinal_entities\",\n",
    "    description=\"Presence of ordinal entities in the essay\",\n",
    "    cmd=\"contains_ordinal(essay)\"\n",
    ")\n",
    "\n",
    "# Create a group that checks for the presence of ordinal entities\n",
    "ordinal_group = Group.create(\n",
    "    name=\"ordinal\",\n",
    "    description=\"ordinal entity detected\",\n",
    "    cmd=\"attr:ordinal_entities == TRUE\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "quantity_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2169c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.0, 'accuracy_PE_Acceptable': 'None', 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.7142857142857143, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.42857142857142855, 'accuracy_KE_NotFound': 0.14285714285714285, 'accuracy_LCE_Acceptable': 0.4, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.2857142857142857}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "\n",
    "for key in ordinal_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc5675",
   "metadata": {},
   "source": [
    "## Group - Number of Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb3a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_adjectives(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of adjectives in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f6b84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_adjectives):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: num_adjectives_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[[BuildBlockOp](attr):num_adjectives_in_essay, 10.0]\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:num_adjectives_in_essay > 10\n",
       "\tCOUNT\t: 61"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_adjectives_in_essay\",\n",
    "    description=\"Number of adjectives among essay targets\",\n",
    "    cmd=\"num_adjectives(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "adj_count_group = Group.create(\n",
    "    name=\"adjective_count_group\",\n",
    "    description=\"Group for counting adjectives in the essay\",\n",
    "    cmd=\"attr:num_adjectives_in_essay > 10\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "adj_count_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1b63180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.03278688524590164, 'accuracy_PE_Acceptable': 0.0, 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.8205128205128205, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.7096774193548387, 'accuracy_KE_NotFound': 0.13333333333333333, 'accuracy_LCE_Acceptable': 0.2682926829268293, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in adj_count_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1b7b7",
   "metadata": {},
   "source": [
    "## Group - Number of Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5b047b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_verbs(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0241bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_verbs):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: num_verbs_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[[BuildBlockOp](attr):num_verbs_in_essay, 40.0]\n",
      "INFO:errudite.builts.group:Created group: verb_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](verb_count_group): \n",
       "\tCMD\t: attr:num_verbs_in_essay > 40\n",
       "\tCOUNT\t: 20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_verbs_in_essay\",\n",
    "    description=\"Number of verbs among essay targets\",\n",
    "    cmd=\"num_verbs(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 20 verbs in the essay\n",
    "verb_count_group = Group.create(\n",
    "    name=\"verb_count_group\",\n",
    "    description=\"Group for counting verbs in the essay\",\n",
    "    cmd=\"attr:num_verbs_in_essay > 40\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "verb_count_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c396279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.05, 'accuracy_PE_Acceptable': 'None', 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.7142857142857143, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.7272727272727273, 'accuracy_KE_NotFound': 0.2222222222222222, 'accuracy_LCE_Acceptable': 0.2857142857142857, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.4444444444444444}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in verb_count_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31af30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some general stats on groups\n",
    "# Group.eval_stats(\n",
    "#     filtered_instances=adj_count_group.get_instances(),\n",
    "#     # this will automatically call the default model we got\n",
    "#     model=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14c325",
   "metadata": {},
   "source": [
    "## Group - Number of Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93a6a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register num_verbs as PrimFunc: Overwritting name already in use for num_verbs.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_verbs(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "17af39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_verbs):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing num_verbs_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: num_verbs_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[[BuildBlockOp](attr):num_verbs_in_essay, 50.0]\n",
      "WARNING:errudite.utils.store:Storing verb_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: verb_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](verb_count_group): \n",
       "\tCMD\t: attr:num_verbs_in_essay > 50\n",
       "\tCOUNT\t: 22"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_verbs_in_essay\",\n",
    "    description=\"Number of verbs among essay targets\",\n",
    "    cmd=\"num_verbs(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 20 verbs in the essay\n",
    "verb_count_group = Group.create(\n",
    "    name=\"verb_count_group\",\n",
    "    description=\"Group for counting verbs in the essay\",\n",
    "    cmd=\"attr:num_verbs_in_essay > 50\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "verb_count_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4b7daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.045454545454545456, 'accuracy_PE_Acceptable': 0.0, 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.6363636363636364, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.6666666666666666, 'accuracy_KE_NotFound': 0.15384615384615385, 'accuracy_LCE_Acceptable': 0.25, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.6153846153846154}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in verb_count_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed7dd2",
   "metadata": {},
   "source": [
    "## Group - Contains Location Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "67db84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register contains_loc as PrimFunc: Overwritting name already in use for contains_loc.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def contains_loc(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Detect the presence of location entities ('bottom' or 'top') in the essay.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        target_str=STRING(target)\n",
    "        #tokens = STRING(target).tokens\n",
    "        if \"bottom\" in target_str or \"top\" in target_str:\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e8610139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_loc):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing contains_loc_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: contains_loc_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):contains_loc_in_essay, True]\n",
      "WARNING:errudite.utils.store:Storing adjective_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:contains_loc_in_essay==TRUE \n",
       "\tCOUNT\t: 57"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"contains_loc_in_essay\",\n",
    "    description=\"Number of locations among essay targets\",\n",
    "    cmd=\"contains_loc(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "contains_loc_group = Group.create(\n",
    "    name=\"adjective_count_group\",\n",
    "    description=\"Group for counting locations in the essay\",\n",
    "    cmd=\"attr:contains_loc_in_essay==TRUE \",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "contains_loc_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "af599e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counts': {'correct': 3, 'incorrect': 54},\n",
       " 'stats': {'coverage': 0.75,\n",
       "  'error_coverage': 0.7397260273972602,\n",
       "  'local_error_rate': 0.9473684210526315,\n",
       "  'global_error_rate': 0.7105263157894737}}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some general stats on groups\n",
    "Group.eval_stats(\n",
    "    filtered_instances=contains_loc_group.get_instances(),\n",
    "    # this will automatically call the default model we got\n",
    "    model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87591e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_loc_group.visualize_models(models=[\"FLAN_T5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c0b393aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.05263157894736842, 'accuracy_PE_Acceptable': 0.0, 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.825, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.7, 'accuracy_KE_NotFound': 0.07407407407407407, 'accuracy_LCE_Acceptable': 0.2631578947368421, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.5185185185185185}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in contains_loc_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40595ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from errudite.build_blocks import PrimFunc\n",
    "# from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# @PrimFunc.register()\n",
    "# def contains_loc(target: 'Target') -> int:\n",
    "#     \"\"\"\n",
    "#     Detect the presence of location entities ('bottom' or 'top') in the essay.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Access the tokens associated with the Target\n",
    "#         answer_type(target)\n",
    "#         #target_str=STRING(target)\n",
    "#         #tokens = STRING(target).tokens\n",
    "#         #if \"bottom\" in target_str or \"top\" in target_str:\n",
    "#          #   return True\n",
    "#     except Exception as e:\n",
    "#         ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "#         raise ex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "12279331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from errudite.builts import Attribute, Group\n",
    "\n",
    "# # Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# # Create an attribute that counts the adjectives among essay targets\n",
    "# attr = Attribute.create(\n",
    "#     name=\"contains_loc_in_essay\",\n",
    "#     description=\"Number of adjectives among essay targets\",\n",
    "#     cmd=\"contains_loc(essay)\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create a group to check if there are more than 5 adjectives in the essay\n",
    "# contains_loc_group = Group.create(\n",
    "#     name=\"adjective_count_group\",\n",
    "#     description=\"Group for counting adjectives in the essay\",\n",
    "#     cmd=\"attr:contains_loc_in_essay==TRUE \",\n",
    "#     attr_hash=Attribute.store_hash(),\n",
    "#     group_hash=Group.store_hash()\n",
    "# )\n",
    "\n",
    "# # You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "# contains_loc_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2958cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_group_list = []\n",
    "for key in verb_count_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1075505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register num_assertions as PrimFunc: Overwritting name already in use for num_assertions.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_assertions(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        # Filter statements starting with \"I believe\" or \"I think\"\n",
    "        filtered_statements = [sent.text for sent in doc.sents if sent.text.lower().startswith(\"i believe\") or sent.text.lower().startswith(\"i think\")]\n",
    "        #print(filtered_statements)\n",
    "        return filtered_statements\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [  ]: {e}\")\n",
    "        raise ex\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b573ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_assertions):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing contains_loc_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: contains_loc_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):contains_loc_in_essay, True]\n",
      "WARNING:errudite.utils.store:Storing adjective_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:contains_loc_in_essay==TRUE \n",
       "\tCOUNT\t: 0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"contains_loc_in_essay\",\n",
    "    description=\"Number of adjectives among essay targets\",\n",
    "    cmd=\"num_assertions(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "contains_loc_group = Group.create(\n",
    "    name=\"adjective_count_group\",\n",
    "    description=\"Group for checking assertions in the essay\",\n",
    "    cmd=\"attr:contains_loc_in_essay==TRUE \",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "contains_loc_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8e220175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.045454545454545456, 'accuracy_PE_Acceptable': 0.0, 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.6363636363636364, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.6666666666666666, 'accuracy_KE_NotFound': 0.15384615384615385, 'accuracy_LCE_Acceptable': 0.25, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.6153846153846154}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in verb_count_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f4a2849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 1.0\n",
      "median : 2.0\n",
      "Upper Quartile : 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [4, 1, 1, 3, 3, 1, 4, 1, 4, 5, 0, 2, 1, 3, 0, 1, 0, 3, 4, 1, 0, 3, 0, 5, 0, 2, 3, 0, 0, 1, 4, 2, 0, 0, 8, 1, 1, 2, 0, 3, 0, 1, 4, 0, 0, 0, 1, 0, 1, 2, 2, 3, 2, 6, 2, 2, 1, 2, 2, 1, 3, 8, 2, 2, 11, 6, 3, 3, 2, 10, 2, 1, 1, 3, 2, 0]\n",
    "\n",
    "# Calculate the lower quartile (Q1)\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "\n",
    "print(\"Upper Quartile :\", upper_quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2eba558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negations: [\"n't\", 'not']\n",
      "Number of negations: 2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I don't believe that understanding potential energy is not crucial. However, I think kinetic energy is equally important.\"\n",
    "\n",
    "# Process the text using SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Identify negations\n",
    "negations = [token.text for token in doc if token.dep_ == \"neg\"]\n",
    "\n",
    "# Count the number of negations\n",
    "num_negations = len(negations)\n",
    "\n",
    "# Print the results\n",
    "print(\"Negations:\", negations)\n",
    "print(\"Number of negations:\", num_negations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b865e9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register num_negations as PrimFunc: Overwritting name already in use for num_negations.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_negations(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        # Identify negations\n",
    "        negations = [token.text for token in doc if token.dep_ == \"neg\"]\n",
    "        # Count the number of negations\n",
    "        num_negations = len(negations)\n",
    "        return num_negations\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [  ]: {e}\")\n",
    "        raise ex\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8678839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_negations):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing contains_negation_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: contains_negation_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[[BuildBlockOp](attr):contains_negation_in_essay, 3.0]\n",
      "WARNING:errudite.utils.store:Storing contains_negation_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: contains_negation_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](contains_negation_group): \n",
       "\tCMD\t: attr:contains_negation_in_essay  >= 3 \n",
       "\tCOUNT\t: 26"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"contains_negation_in_essay\",\n",
    "    description=\"Number of nagations among essay targets\",\n",
    "    cmd=\"num_negations(essay)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "contains_negation_group = Group.create(\n",
    "    name=\"contains_negation_group\",\n",
    "    description=\"Group for checking negations in the essay\",\n",
    "    cmd=\"attr:contains_negation_in_essay  >= 3 \",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "contains_negation_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e55eba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'FLAN_T5', 'perform': {'accuracy': 0.038461538461538464, 'accuracy_PE_Acceptable': 'None', 'accuracy_PE_Unacceptable': 'None', 'accuracy_PE_Insufficient': 0.7222222222222222, 'accuracy_PE_NotFound': 0.0, 'accuracy_KE_Acceptable': 'None', 'accuracy_KE_Unacceptable': 'None', 'accuracy_KE_Insufficient': 0.5, 'accuracy_KE_NotFound': 0.2, 'accuracy_LCE_Acceptable': 0.4117647058823529, 'accuracy_LCE_Unacceptable': 'None', 'accuracy_LCE_Insufficient': 0.0, 'accuracy_LCE_NotFound': 0.4}}\n"
     ]
    }
   ],
   "source": [
    "count_group_list = []\n",
    "for key in contains_negation_group.get_instances():\n",
    "    count_group_list.append(Instance.get(key))\n",
    "\n",
    "predictor.evaluate_performance(count_group_list)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687472ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
