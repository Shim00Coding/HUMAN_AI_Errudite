{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "d739a51e-2db6-4a7e-9cb2-f2bda498ba51",
=======
   "execution_count": 5,
   "id": "b3471b3f",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def import_sys():\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "import_sys()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "7d78798d-2057-4c18-9072-81eb3d58b15f",
=======
   "execution_count": 6,
   "id": "a83000eb",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.file_utils:Local path not yet exist, but still parsed: C:\\Users\\dongh\\errudite\\errudite_relationship\\tutorials\\caches\\vocab.pkl\n",
      "WARNING:errudite.processor.spacy_annotator:(2, 'No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from errudite.io import DatasetReader\n",
    "from errudite.utils import normalize_file_path, accuracy_score\n",
    "from errudite.targets.instance import Instance\n",
    "from errudite.targets.target import Target\n",
    "from errudite.targets.label import Label, PredefinedLabel"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "6245d0ca-452d-4ee2-a17d-c848f1d61ed4",
=======
   "execution_count": 7,
   "id": "abe1052a",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "INFO:pytorch_transformers.modeling_bert:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "INFO:pytorch_transformers.modeling_xlnet:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from allennlp.data.instance import Instance\n",
    "@DatasetReader.register(\"STE_relationship\")\n",
    "class STEReader_relationship(DatasetReader):\n",
    "    # ... (previous code)\n",
    "    @overrides\n",
    "    def _read(self, file_path: str, lazy: bool, sample_size: int) -> List[Instance]:\n",
    "        \"\"\"\n",
    "        Returns a list containing all the instances in the specified dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            The path of the input data file.\n",
    "        lazy : bool\n",
    "            If ``lazy==True``, only run the tokenization, does not compute the linguistic\n",
    "            features like POS, NER. By default False\n",
    "        sample_size : int\n",
    "            If sample size is set, only load this many of instances, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Instance]\n",
    "            The instance list.\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        \n",
    "        # Read data from the CSV file\n",
    "        df = pd.read_csv(normalize_file_path(file_path))\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            instance = self._text_to_instance(f'q:{idx}', row)\n",
    "            if instance is not None:\n",
    "                instances.append(instance)\n",
    "            if sample_size and idx >= sample_size:\n",
    "                break\n",
    "\n",
    "        return instances\n",
    "\n",
    "    @overrides\n",
    "    def _text_to_instance(self, id: str, row) -> Instance:\n",
    "        # Extract information from DataFrame columns\n",
    "        essay_text = row['Essay']\n",
    "        cu0_actual = row['CU0_Actual']\n",
    "        cu0_predicted = row['CU0_Predicted']\n",
    "        cu5_actual = row['CU5_Actual']\n",
    "        cu5_predicted = row['CU5_Predicted']\n",
    "\n",
    "        # Create instance with extracted information\n",
    "        essay = Target(qid=id, text=essay_text, vid=0, metas={'type': 'essays'})\n",
    "        groundtruth_cu0 = PredefinedLabel(model='groundtruth', qid=id, text=cu0_actual, vid=0)\n",
    "        predict_cu0=PredefinedLabel(model='groundtruth', qid=id, text=cu0_predicted, vid=0)\n",
    "        groundtruth_cu5 = PredefinedLabel(model='groundtruth', qid=id, text=cu5_actual, vid=0)\n",
    "        predict_cu5=PredefinedLabel(model='groundtruth', qid=id, text=cu5_predicted, vid=0)\n",
    "        return self.create_instance(\n",
    "            id,\n",
    "            essay=essay,\n",
    "            groundtruth_cu0=groundtruth_cu0,\n",
    "            predict_cu0=predict_cu0,\n",
    "            groundtruth_cu5=groundtruth_cu5,\n",
    "            predict_cu5=predict_cu5\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "2c98faf3-a994-4ea6-b8eb-16b95c1c999b",
=======
   "execution_count": 8,
   "id": "17b4a94f",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.utils.file_utils:Errudite cache folder selected: ./ste_caches\n"
     ]
    }
   ],
   "source": [
    "from errudite.io import DatasetReader\n",
    "\n",
    "cache_folder_path = \"./ste_caches\"\n",
    "reader = DatasetReader.by_name(\"STE_relationship\")(cache_folder_path=cache_folder_path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "1238e312-d090-4778-8516-7aef7904c90e",
=======
   "execution_count": 9,
   "id": "6aa8a11d",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.io.dataset_reader:Reading instances from lines in file at: relationship_pred_vs_actuals.csv\n",
      "INFO:__main__:Reading instances from lines in file at: relationship_pred_vs_actuals.csv\n",
<<<<<<< HEAD
      "175it [00:12, 13.53it/s]\n"
=======
      "175it [00:08, 19.87it/s]\n"
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     ]
    }
   ],
   "source": [
    "# read the raw data!\n",
    "instances = reader.read(\n",
    "    # The path of the input data file. We are using the first 100 rows from the SNLI dev set.\n",
    "    file_path='relationship_pred_vs_actuals.csv', \n",
    "    # If sample size is set, only load this many of instances, by default None.\n",
    "    sample_size=175)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "425b020b-3f77-43ad-9c3b-ccbcdf96e1a7",
=======
   "execution_count": 10,
   "id": "a3550eae",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instance] [InstanceKey(qid='q:1', vid=0)]\n",
      "[essay]\tMy recommendation for the roller coaster would be that for the initial drop to be at the tallest it can to get a good momentum to keep your ride going. Following that your hill shouldn't be taller than your initial drop a the start because then the car wont be able to make it over. Lastly your mass shouldn't be huge but not small so right in the middle so the car can do the hills and all the stuff you want it to safely.\n",
      "\n",
      "The greater amount of height there was more total energy released in the end, But with friction the total amount of energy would go down.\n",
      "As the car goes down and comes to the stop PE goes all the way to 0 meanwhile KE goes up to a higher number.\n",
      "The PE and KE depend on the height of the drop because the more the height is the more the PE will be a the top of the ride and the more the KE will be at the bottom of the ride. \n",
      "[groundtruth_cu0]\t0\tgroundtruth\t{}\n",
      "[predict_cu0]\t1\tgroundtruth\t{}\n",
      "[groundtruth_cu5]\t1\tgroundtruth\t{}\n",
      "[predict_cu5]\t1\tgroundtruth\t{}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances[1].show_instance()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "65b8b5db-1521-4647-a965-f39b9b59c466",
=======
   "execution_count": 11,
   "id": "9baa7b6e",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import sys\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "d7148ea9-16a8-4a8c-b43d-6bd115dad06b",
=======
   "execution_count": 12,
   "id": "c6e06ef0",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import Union, List\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "def import_sys():\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "import_sys()\n",
    "from errudite.utils.helpers import convert_doc\n",
    "from errudite.utils.check import DSLValueError\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "id": "fc059587-52ac-47a7-ad24-03b7a436974b",
=======
   "execution_count": 13,
   "id": "521e2f25",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Build the instance store hash\n",
    "from errudite.targets.instance import Instance\n",
    "instance_hash, instance_hash_rewritten, qid_hash = Instance.build_instance_hashes(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd93bc",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "3c6a0136-07fe-41d3-9687-bc85f290d12f",
=======
   "execution_count": 45,
   "id": "4f54f50b",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance(group):\n",
    "\n",
    "    cu0_count=0\n",
    "    cu5_count=0\n",
    "    len_group=0\n",
    "    for key in group.get_instances():\n",
    "        len_group+=1\n",
    "        instance =Instance.get(key)\n",
    "        if(int(instance.get_entry(\"groundtruth_cu0\").label)== int(instance.get_entry(\"predict_cu0\").label)):\n",
    "            cu0_count+=1 \n",
    "        if(int(instance.get_entry(\"groundtruth_cu5\").label)== int(instance.get_entry(\"predict_cu5\").label)):\n",
    "            cu5_count+=1 \n",
<<<<<<< HEAD
    "    if(len_group == 0):\n",
    "        print(\"None\")\n",
=======
    "    if (len_group == 0):\n",
    "        print('None')\n",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
    "        return\n",
    "    cu0_acc=cu0_count/len_group\n",
    "    cu5_acc=cu5_count/len_group\n",
    "    print(f'cu0 accuracy: {cu0_acc}\\ncu5_accuracy: {cu5_acc}')\n",
<<<<<<< HEAD
    "    return "
=======
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c801b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_performance_original_instance(group):\n",
    "\n",
    "    cu0_count=0\n",
    "    cu5_count=0\n",
    "    len_group=0\n",
    "    for key in group:\n",
    "        len_group+=1\n",
    "        if(int(key.get_entry(\"groundtruth_cu0\").label)== int(key.get_entry(\"predict_cu0\").label)):\n",
    "            cu0_count+=1 \n",
    "        if(int(key.get_entry(\"groundtruth_cu5\").label)== int(key.get_entry(\"predict_cu5\").label)):\n",
    "            cu5_count+=1 \n",
    "    if (len_group == 0):\n",
    "        print('None')\n",
    "        return\n",
    "    cu0_acc=cu0_count/len_group\n",
    "    cu5_acc=cu5_count/len_group\n",
    "    print(f'cu0 accuracy: {cu0_acc}\\ncu5_accuracy: {cu5_acc}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a99a868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9257142857142857\n",
      "cu5_accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "eval_performance_original_instance(instances)"
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78dc1c",
   "metadata": {},
   "source": [
    "## Group - Length of the Essay"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "id": "d5e6fa3e-17f4-4060-b41f-35c5b6d3efea",
=======
   "execution_count": 49,
   "id": "5ff03dd7",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(length):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: len_entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 251.5\n",
      "median : 361.0\n",
      "Upper Quartile : 464.0\n"
     ]
    }
   ],
   "source": [
    "from errudite.builts import Group\n",
    "from errudite.builts import Attribute\n",
    "\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"len_entities\",\n",
    "    description=\"length entities in the essay\",\n",
    "    cmd=\"length(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "id": "ea76d2d6-0fb5-45ae-a84c-56b478ada063",
=======
   "execution_count": 60,
   "id": "4c559c44",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[FuncOp(length):[ArgOp:essay]+[], 248.5]\n",
=======
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](<):[FuncOp(length):[ArgOp:essay]+[], 251.5]\n",
      "WARNING:errudite.utils.store:Storing length in Group: Overwritting name already in use.\n",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
      "INFO:errudite.builts.group:Created group: length\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](length): \n",
       "\tCMD\t: length(essay) < 251.5\n",
       "\tCOUNT\t: 44"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 60,
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len_group = Group.create(\n",
    "    # The name of the attribute\n",
    "    name=\"length\",\n",
    "    # the description of the attribute\n",
    "    description=\"length greater than 10\",\n",
    "    # All the previously created attributes and groups \n",
    "    # can be used and queried, as long as we serve the \n",
    "    # stored attributes and groups as part of the inputs.\n",
    "    cmd=\"length(essay) < 251.5\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "len_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "id": "f7e05103-7a4c-41e8-9c39-c4e415d78dc0",
=======
   "execution_count": 61,
   "id": "70627a15",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9318181818181818\n",
      "cu5_accuracy: 0.8409090909090909\n"
     ]
    }
   ],
   "source": [
    "eval_performance(len_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef61ca",
   "metadata": {},
   "source": [
    "## Group - Quantitative Value"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "id": "afc39405-e8e4-4241-9e59-d6446adc8e9b",
=======
   "execution_count": 68,
   "id": "3353e536",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register contains_quantity as PrimFunc: Overwritting name already in use for contains_quantity.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def contains_quantity(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the presence of quantity entities in the essay.\n",
    "    quantity entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "\n",
    "    contains='QUANTITY' in entities\n",
    "\n",
    "    # Check if 'bottom' or 'top' is present in the extracted entity types\n",
    "    #print(contains)\n",
    "    return contains\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "id": "6632b3dc-6b29-4cd8-8cc8-c09858031a48",
=======
   "execution_count": 69,
   "id": "d8ab124a",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_quantity):[ArgOp:essay]+[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.store:Storing quantity_entities in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: quantity_entities\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):quantity_entities, True]\n",
      "WARNING:errudite.utils.store:Storing quantity in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: quantity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](quantity): \n",
       "\tCMD\t: attr:quantity_entities == TRUE\n",
       "\tCOUNT\t: 138"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 69,
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"quantity_entities\",\n",
    "    description=\"Presence of quantity entities in the essay\",\n",
    "    cmd=\"contains_quantity(essay)\"\n",
    ")\n",
    "\n",
    "# Create a group that checks for the presence of location entities\n",
    "quantity_group = Group.create(\n",
    "    name=\"quantity\",\n",
    "    description=\"quantity entity detected\",\n",
    "    cmd=\"attr:quantity_entities == TRUE\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "quantity_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "id": "4c95e422-faba-4600-bc12-d65c1a032cd3",
=======
   "execution_count": 70,
   "id": "e85d634b",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.927536231884058\n",
      "cu5_accuracy: 0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "eval_performance(quantity_group)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "id": "6d296306-8deb-4331-a3b3-493fe93c13d6",
=======
   "execution_count": 62,
   "id": "2efd9e35",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_quantity):[ArgOp:essay]+[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.store:Storing quantity_entities in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: quantity_entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.builts.attribute:Created attr: quantity_entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 2.0\n",
      "median : 6.0\n",
      "Upper Quartile : 12.0\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_quantity(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the number of quantity entities in the essay.\n",
    "    quantity entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "    #print(docs)\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "    count = entities.count('QUANTITY')\n",
    "\n",
    "    return count\n",
    "\n",
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"quantity_entities\",\n",
    "    description=\"number of quantity entities in the essay\",\n",
    "    cmd=\"num_quantity(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "id": "0a8fe274-7c2f-4864-9cf7-e439c33ddf82",
=======
   "execution_count": 69,
   "id": "7b46c88b",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[[BuildBlockOp](attr):quantity_entities, 12.0]\n",
      "WARNING:errudite.utils.store:Storing num_quantity in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: num_quantity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](num_quantity): \n",
       "\tCMD\t: attr:quantity_entities >= 12\n",
       "\tCOUNT\t: 48"
      ]
     },
<<<<<<< HEAD
     "execution_count": 18,
=======
     "execution_count": 69,
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a group that checks for the presence of location entities\n",
    "num_quantity_group = Group.create(\n",
    "    name=\"num_quantity\",\n",
    "    description=\"quantity entity detected\",\n",
    "    cmd=\"attr:quantity_entities >= 12\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "num_quantity_group\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "id": "f3e8a8bf-ef12-44fb-90e9-bb3b4622a1c8",
=======
   "execution_count": 70,
   "id": "2adeb153",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.8958333333333334\n",
      "cu5_accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "eval_performance(num_quantity_group)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "id": "4df2381c-c05f-41c5-b812-cab823ee2894",
=======
   "execution_count": 74,
   "id": "97e3c335",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register contains_ordinal as PrimFunc: Overwritting name already in use for contains_ordinal.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import linguistic\n",
    "\n",
    "@PrimFunc.register()\n",
    "def contains_ordinal(docs: Union['Target', Span]) -> bool:\n",
    "    \"\"\"\n",
    "    Detect the presence of ordinal entities in the essay.\n",
    "    ordinal entity: measurements or counts.\n",
    "    \"\"\"\n",
    "    # Use the linguistic function to extract entity types\n",
    "    entities = linguistic(docs, label='ent_type')\n",
    "\n",
    "    \n",
    "    contains='ORDINAL' in entities\n",
    "    # Check if 'bottom' or 'top' is present in the extracted entity types\n",
    "    #print(contains)\n",
    "    return contains\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "id": "1a361063-14be-4218-800d-b0a51f0d2225",
=======
   "execution_count": 76,
   "id": "4079eb5b",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_ordinal):[ArgOp:essay]+[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.builts.attribute:Created attr: ordinal_entities\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):ordinal_entities, True]\n",
      "INFO:errudite.builts.group:Created group: ordinal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.store:Storing ordinal_entities in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: ordinal_entities\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):ordinal_entities, True]\n",
      "WARNING:errudite.utils.store:Storing ordinal in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: ordinal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](quantity): \n",
       "\tCMD\t: attr:quantity_entities == TRUE\n",
       "\tCOUNT\t: 138"
      ]
     },
<<<<<<< HEAD
     "execution_count": 21,
=======
     "execution_count": 76,
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"ordinal_entities\",\n",
    "    description=\"Presence of ordinal entities in the essay\",\n",
    "    cmd=\"contains_ordinal(essay)\"\n",
    ")\n",
    "\n",
    "# Create a group that checks for the presence of ordinal entities\n",
    "ordinal_group = Group.create(\n",
    "    name=\"ordinal\",\n",
    "    description=\"ordinal entity detected\",\n",
    "    cmd=\"attr:ordinal_entities == TRUE\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "quantity_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "id": "a269c7e6-1fcf-42f6-9d54-40937d14759b",
=======
   "execution_count": 77,
   "id": "53ae8a59",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.927536231884058\n",
      "cu5_accuracy: 0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "eval_performance(quantity_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a01f0",
   "metadata": {},
   "source": [
    "## Group - Number of Adjectives"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "id": "14fe4f4e-bfbf-4029-b856-ccecb16bfc07",
=======
   "execution_count": 71,
   "id": "7d550edd",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_adjectives(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of adjectives in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "id": "87912299-83c3-45ef-9fd0-9db9fa4d559b",
=======
   "execution_count": 75,
   "id": "3d8331ee",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_adjectives):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing num_adjectives_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: num_adjectives_in_essay\n"
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:num_adjectives_in_essay > 10\n",
       "\tCOUNT\t: 155"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 15.5\n",
      "median : 24.0\n",
      "Upper Quartile : 33.5\n"
     ]
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_adjectives_in_essay\",\n",
    "    description=\"Number of adjectives among essay targets\",\n",
    "    cmd=\"num_adjectives(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8519ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[[BuildBlockOp](attr):num_adjectives_in_essay, 33.5]\n",
      "WARNING:errudite.utils.store:Storing adjective_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:num_adjectives_in_essay >= 33.5\n",
       "\tCOUNT\t: 44"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "adj_count_group = Group.create(\n",
    "    name=\"adjective_count_group\",\n",
    "    description=\"Group for counting adjectives in the essay\",\n",
    "    cmd=\"attr:num_adjectives_in_essay >= 33.5\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "adj_count_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "id": "8614a58d-9819-4b27-b661-1a01387a0366",
=======
   "execution_count": 83,
   "id": "12ef6b9a",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9772727272727273\n",
      "cu5_accuracy: 0.7954545454545454\n"
     ]
    }
   ],
   "source": [
    "eval_performance(adj_count_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ec32f",
   "metadata": {},
   "source": [
    "## Group - Number of Verbs"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "id": "2653177b-95bc-4b17-9fc5-8677093aab5c",
=======
   "execution_count": 84,
   "id": "b233e7db",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_verbs(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "id": "fdc6119f-d7a5-43cb-b7a6-9d4509a41a0f",
=======
   "execution_count": 85,
   "id": "07de0da6",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_verbs):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: num_verbs_in_essay\n"
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "[Group](verb_count_group): \n",
       "\tCMD\t: attr:num_verbs_in_essay > 40\n",
       "\tCOUNT\t: 131"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 40.5\n",
      "median : 56.0\n",
      "Upper Quartile : 76.5\n"
     ]
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_verbs_in_essay\",\n",
    "    description=\"Number of verbs among essay targets\",\n",
    "    cmd=\"num_verbs(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "faec0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[[BuildBlockOp](attr):num_verbs_in_essay, 76.5]\n",
      "WARNING:errudite.utils.store:Storing verb_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: verb_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](verb_count_group): \n",
       "\tCMD\t: attr:num_verbs_in_essay >= 76.5\n",
       "\tCOUNT\t: 44"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a group to check if there are more than 20 verbs in the essay\n",
    "verb_count_group = Group.create(\n",
    "    name=\"verb_count_group\",\n",
    "    description=\"Group for counting verbs in the essay\",\n",
    "    cmd=\"attr:num_verbs_in_essay >= 76.5\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "verb_count_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "id": "ce245fe3-f4a2-4c5c-91a6-86425954a0aa",
=======
   "execution_count": 94,
   "id": "f9e40b1c",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9318181818181818\n",
      "cu5_accuracy: 0.7954545454545454\n"
     ]
    }
   ],
   "source": [
    "eval_performance(verb_count_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab92000",
   "metadata": {},
   "source": [
    "## Group - Number of Nouns"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "id": "9c00866d-b798-4b22-8560-bd18cee887c5",
=======
   "execution_count": 96,
   "id": "c613d610",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.registrable:Register num_nouns as PrimFunc: Overwritting name already in use for num_nouns.\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_nouns(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        adjectives = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "        return len(adjectives)\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "        raise ex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "id": "b21f8c20-1c7f-4fb3-b783-e7b10baf3b03",
=======
   "execution_count": 97,
   "id": "e011314a",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_nouns):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: num_nouns_in_essay\n"
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "[Group](verb_count_group): \n",
       "\tCMD\t: attr:num_verbs_in_essay > 50\n",
       "\tCOUNT\t: 126"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 47.5\n",
      "median : 69.0\n",
      "Upper Quartile : 92.0\n"
     ]
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"num_nouns_in_essay\",\n",
    "    description=\"Number of nouns among essay targets\",\n",
    "    cmd=\"num_nouns(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ee6a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](and):[[BinOp](>=):[[BuildBlockOp](attr):num_nouns_in_essay, 69.0], [BinOp](<):[[BuildBlockOp](attr):num_nouns_in_essay, 92.0]]\n",
      "WARNING:errudite.utils.store:Storing noun_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: noun_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](noun_count_group): \n",
       "\tCMD\t: attr:num_nouns_in_essay >= 69 and attr:num_nouns_in_essay < 92\n",
       "\tCOUNT\t: 39"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a group to check if there are more than 20 verbs in the essay\n",
    "noun_count_group = Group.create(\n",
    "    name=\"noun_count_group\",\n",
    "    description=\"Group for counting nouns in the essay\",\n",
    "    cmd=\"attr:num_nouns_in_essay >= 69 and attr:num_nouns_in_essay < 92\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "noun_count_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "id": "01baf8ab-2486-4c49-b079-b872f39f344d",
=======
   "execution_count": 104,
   "id": "3b4678ac",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9487179487179487\n",
      "cu5_accuracy: 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "eval_performance(noun_count_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fd6e7",
   "metadata": {},
   "source": [
    "## Group - Contains Location Entity / Assertions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "id": "e872a5bf-f3e8-4ad3-952f-6ebbf394c006",
=======
   "execution_count": 87,
   "id": "995e4ef2",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "# from errudite.build_blocks import PrimFunc\n",
    "# from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# @PrimFunc.register()\n",
    "# def contains_loc(target: 'Target') -> int:\n",
    "#     \"\"\"\n",
    "#     Detect the presence of location entities ('bottom' or 'top') in the essay.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Access the tokens associated with the Target\n",
    "#         target_str=STRING(target)\n",
    "#         #tokens = STRING(target).tokens\n",
    "#         if \"bottom\" in target_str or \"top\" in target_str:\n",
    "#             return True\n",
    "#     except Exception as e:\n",
    "#         ex = Exception(f\"Unknown exception from [num_adjectives]: {e}\")\n",
    "#         raise ex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
   "id": "bbfe46cf-9069-42de-8532-922ccd391d8b",
=======
   "execution_count": 88,
   "id": "caff7bf0",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(contains_loc):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: contains_loc_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):contains_loc_in_essay, True]\n",
      "WARNING:errudite.utils.store:Storing adjective_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:contains_loc_in_essay==TRUE \n",
       "\tCOUNT\t: 142"
      ]
     },
<<<<<<< HEAD
     "execution_count": 33,
=======
     "execution_count": 88,
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from errudite.builts import Attribute, Group\n",
    "\n",
    "# # Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# # Create an attribute that counts the adjectives among essay targets\n",
    "# attr = Attribute.create(\n",
    "#     name=\"contains_loc_in_essay\",\n",
    "#     description=\"Number of locations among essay targets\",\n",
    "#     cmd=\"contains_loc(essay)\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create a group to check if there are more than 5 adjectives in the essay\n",
    "# contains_loc_group = Group.create(\n",
    "#     name=\"adjective_count_group\",\n",
    "#     description=\"Group for counting locations in the essay\",\n",
    "#     cmd=\"attr:contains_loc_in_essay==TRUE \",\n",
    "#     attr_hash=Attribute.store_hash(),\n",
    "#     group_hash=Group.store_hash()\n",
    "# )\n",
    "\n",
    "# # You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "# contains_loc_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "id": "801374cf-96b0-4b78-b7ba-f53aa0ff42e5",
=======
   "execution_count": 89,
   "id": "5a86faa4",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9154929577464789\n",
      "cu5_accuracy: 0.8450704225352113\n"
     ]
    }
   ],
   "source": [
    "# eval_performance(contains_loc_group)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "id": "cce84ae6-f146-4a12-8453-9b6b0e9c2d15",
=======
   "execution_count": 90,
   "id": "38cab803",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "# from errudite.build_blocks import PrimFunc\n",
    "# from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# @PrimFunc.register()\n",
    "# def num_assertions(target: 'Target') -> int:\n",
    "#     \"\"\"\n",
    "#     Count the number of verbs in a given target.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Access the tokens associated with the Target\n",
    "#         #print(STRING(target))\n",
    "#         #tokens = STRING(target).tokens\n",
    "#         doc = nlp(STRING(target))\n",
    "#         # Filter statements starting with \"I believe\" or \"I think\"\n",
    "#         filtered_statements = [sent.text for sent in doc.sents if sent.text.lower().startswith(\"i believe\") or sent.text.lower().startswith(\"i think\")]\n",
    "#         #print(filtered_statements)\n",
    "#         return filtered_statements\n",
    "#     except Exception as e:\n",
    "#         ex = Exception(f\"Unknown exception from [  ]: {e}\")\n",
    "#         raise ex\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "id": "49af7685-fea8-406b-b178-2333e417624d",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 91,
   "id": "fc0305d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_assertions):[ArgOp:essay]+[]\n",
      "WARNING:errudite.utils.store:Storing contains_loc_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: contains_loc_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](==):[[BuildBlockOp](attr):contains_loc_in_essay, True]\n",
      "WARNING:errudite.utils.store:Storing adjective_count_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: adjective_count_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](adjective_count_group): \n",
       "\tCMD\t: attr:contains_loc_in_essay==TRUE \n",
       "\tCOUNT\t: 0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "source": [
    "# from errudite.builts import Attribute, Group\n",
    "\n",
    "# # Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# # Create an attribute that counts the adjectives among essay targets\n",
    "# attr = Attribute.create(\n",
    "#     name=\"contains_loc_in_essay\",\n",
    "#     description=\"Number of adjectives among essay targets\",\n",
    "#     cmd=\"num_assertions(essay)\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create a group to check if there are more than 5 adjectives in the essay\n",
    "# contains_loc_group = Group.create(\n",
    "#     name=\"adjective_count_group\",\n",
    "#     description=\"Group for checking assertions in the essay\",\n",
    "#     cmd=\"attr:contains_loc_in_essay==TRUE \",\n",
    "#     attr_hash=Attribute.store_hash(),\n",
    "#     group_hash=Group.store_hash()\n",
    "# )\n",
    "\n",
    "# # You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "# contains_loc_group"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "id": "5a01f4cd-8980-4e2a-bdcc-d0105961383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_performance(contains_loc_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02e120f2-4fb9-4b20-9c2f-39a9d37ed795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 1.0\n",
      "median : 2.0\n",
      "Upper Quartile : 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [4, 1, 1, 3, 3, 1, 4, 1, 4, 5, 0, 2, 1, 3, 0, 1, 0, 3, 4, 1, 0, 3, 0, 5, 0, 2, 3, 0, 0, 1, 4, 2, 0, 0, 8, 1, 1, 2, 0, 3, 0, 1, 4, 0, 0, 0, 1, 0, 1, 2, 2, 3, 2, 6, 2, 2, 1, 2, 2, 1, 3, 8, 2, 2, 11, 6, 3, 3, 2, 10, 2, 1, 1, 3, 2, 0]\n",
    "\n",
    "# Calculate the lower quartile (Q1)\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "\n",
    "print(\"Upper Quartile :\", upper_quartile)"
=======
   "execution_count": 92,
   "id": "72aa869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# eval_performance(contains_loc_group)"
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
   "id": "2d734d8e-b75d-4f6a-bc63-8d8646931de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negations: [\"n't\", 'not']\n",
      "Number of negations: 2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"I don't believe that understanding potential energy is not crucial. However, I think kinetic energy is equally important.\"\n",
    "\n",
    "# Process the text using SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Identify negations\n",
    "negations = [token.text for token in doc if token.dep_ == \"neg\"]\n",
    "\n",
    "# Count the number of negations\n",
    "num_negations = len(negations)\n",
    "\n",
    "# Print the results\n",
    "print(\"Negations:\", negations)\n",
    "print(\"Number of negations:\", num_negations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4ebf134-893a-492b-8f36-0669e0222ae2",
=======
   "execution_count": 24,
   "id": "b5986f9e",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.linguistic import STRING\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_negations(target: 'Target') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of verbs in a given target.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Access the tokens associated with the Target\n",
    "        #print(STRING(target))\n",
    "        #tokens = STRING(target).tokens\n",
    "        doc = nlp(STRING(target))\n",
    "        # Identify negations\n",
    "        negations = [token.text for token in doc if token.dep_ == \"neg\"]\n",
    "        # Count the number of negations\n",
    "        num_negations = len(negations)\n",
    "        return num_negations\n",
    "    except Exception as e:\n",
    "        ex = Exception(f\"Unknown exception from [  ]: {e}\")\n",
    "        raise ex\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
   "id": "3e9d81fc-f949-4af6-a93e-c909cc4212d3",
=======
   "execution_count": 28,
   "id": "c58550c1",
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_negations):[ArgOp:essay]+[]\n",
<<<<<<< HEAD
      "INFO:errudite.builts.attribute:Created attr: contains_negation_in_essay\n",
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>):[[BuildBlockOp](attr):contains_negation_in_essay, 3.0]\n",
      "INFO:errudite.builts.group:Created group: contains_negation_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](contains_negation_group): \n",
       "\tCMD\t: attr:contains_negation_in_essay  > 3 \n",
       "\tCOUNT\t: 70"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
=======
      "WARNING:errudite.utils.store:Storing contains_negation_in_essay in Attribute: Overwritting name already in use.\n",
      "INFO:errudite.builts.attribute:Created attr: contains_negation_in_essay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 1.0\n",
      "median : 3.0\n",
      "Upper Quartile : 4.0\n"
     ]
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
    }
   ],
   "source": [
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "# Assuming you have already defined the PrimFunc num_adjectives\n",
    "\n",
    "# Create an attribute that counts the adjectives among essay targets\n",
    "attr = Attribute.create(\n",
    "    name=\"contains_negation_in_essay\",\n",
    "    description=\"Number of nagations among essay targets\",\n",
    "    cmd=\"num_negations(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51e9bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](>=):[[BuildBlockOp](attr):contains_negation_in_essay, 4.0]\n",
      "WARNING:errudite.utils.store:Storing contains_negation_group in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: contains_negation_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Group](contains_negation_group): \n",
       "\tCMD\t: attr:contains_negation_in_essay >= 4\n",
       "\tCOUNT\t: 70"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a group to check if there are more than 5 adjectives in the essay\n",
    "contains_negation_group = Group.create(\n",
    "    name=\"contains_negation_group\",\n",
    "    description=\"Group for checking negations in the essay\",\n",
    "    cmd=\"attr:contains_negation_in_essay >= 4\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "# You can now use the 'group' to check if there are more than 5 adjectives in your essay targets.\n",
    "contains_negation_group"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f083792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu0 accuracy: 0.9285714285714286\n",
      "cu5_accuracy: 0.8285714285714286\n"
     ]
    }
   ],
   "source": [
    "eval_performance(contains_negation_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4b5d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: FuncOp(num_token):[ArgOp:essay]+[]\n",
      "INFO:errudite.builts.attribute:Created attr: frequency_of_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Quartile : 0.0\n",
      "median : 1.0\n",
      "Upper Quartile : 2.0\n"
     ]
    }
   ],
   "source": [
    "from errudite.build_blocks import PrimFunc\n",
    "from errudite.build_blocks.prim_funcs.token import token\n",
    "from errudite.builts import Attribute, Group\n",
    "\n",
    "@PrimFunc.register()\n",
    "def num_token(docs: Union['Target', Span]) -> int:\n",
    "    num_token = token(docs, pattern = '(increase, greater, more) NOUN')\n",
    "    return len(num_token)\n",
    "    \n",
    "\n",
    "# Create an attribute based on the location function\n",
    "attr = Attribute.create(\n",
    "    name=\"frequency_of_token\",\n",
    "    description=\"number of token in the essay\",\n",
    "    cmd=\"num_token(essay)\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data=list(attr.get_instances().values())\n",
    "lower_quartile = np.percentile(data, 25)\n",
    "upper_quartile = np.percentile(data, 75)\n",
    "median=np.percentile(data, 50)\n",
    "\n",
    "\n",
    "print(\"Lower Quartile :\", lower_quartile)\n",
    "print(\"median :\",median)\n",
    "print(\"Upper Quartile :\", upper_quartile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bb29f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.build_blocks.wrapper:Parsed: [BinOp](<=):[[BuildBlockOp](attr):frequency_of_token, 0.0]\n",
      "WARNING:errudite.utils.store:Storing token in Group: Overwritting name already in use.\n",
      "INFO:errudite.builts.group:Created group: token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Group](token): \n",
      "\tCMD\t: attr:frequency_of_token <= 0\n",
      "\tCOUNT\t: 59\n",
      "\n",
      "cu0 accuracy: 0.9322033898305084\n",
      "cu5_accuracy: 0.864406779661017\n"
     ]
    }
   ],
   "source": [
    "num_more_group = Group.create(\n",
    "    name=\"token\",\n",
    "    description=\"number of token detected\",\n",
    "    cmd=\"attr:frequency_of_token <= 0\",\n",
    "    attr_hash=Attribute.store_hash(),\n",
    "    group_hash=Group.store_hash()\n",
    ")\n",
    "\n",
    "print(num_more_group)\n",
    "\n",
    "eval_performance(num_more_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c92477",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> ebdbd1349579e1d050b497b3e2a44cb270d6b995
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
